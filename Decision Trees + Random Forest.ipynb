{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the differences between decision trees, bagged trees, and random forests? What is the main advantage of random forests over decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Learn.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "- A decision tree is just a series of IF-ELSE statements (rules). Each path from the root of a decision tree to one of its leaves can be transformed into a rule simply by combining the decisions along the path to form the antecedent, and taking the leafâ€™s class prediction as the consequence\n",
    "\n",
    "- Present a dataset of training examples containing features/predictors and a target (similar to classifiers we have seen earlier).\n",
    "\n",
    "- Train the tree model by making splits for the target using the values of predictors. Which features to use as predictors gets selected following the idea of feature selection and uses measures like \"information gain\" and \"Gini Index\". We shall cover these shortly.\n",
    "\n",
    "- The tree is grown until some stopping criteria is achieved. This could be a set depth of the tree or any other similar measure.\n",
    "\n",
    "- Show a new set of features to the tree, with an unknown class and let the example propagate through a trained tree. The resulting leaf node represents the class prediction for this example datum.\n",
    "\n",
    "\n",
    "#### Baggged Tree\n",
    "- Bagged Tree - The first way to encourage differences among the trees in our forest is to train them on different samples of data. Although more data is generally better, if we gave every tree the entire dataset, we would end up with each tree being exactly the same. Because of this, we instead use Bootstrap Aggregation, or Bagging, to a portion of our data with replacement. We want to have a team where each member has their own individual strengths and weaknesses. Not a team that is all trained the same.\n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "- Random Forest - It doesn't matter how many trees we add to our forest, if they're all the same tree! Trees trained on the same dataset will come out the exact same way every time -- there is no randomness to this algorithm. It doesn't matter if our forest has a million decision trees; if they are all exactly the same, then our performance will be no better than if we just had a single tree.\n",
    "\n",
    "- Process:\n",
    "    - Bag 2/3 of the overall data -- in our example, 2000 rows\n",
    "    - Randomly select a set number of features to use for training each node within this -- in this example, 6 features\n",
    "    - Train the tree on modified dataset, which is now a DataFrame consisting of 2000 rows and 6 columns\n",
    "\n",
    "\n",
    "#### Benefit Of Random Forest\n",
    "- Because this is an ensemble algorithm, the model is naturally resistant to noise and variance in the data, and generally tends to perform quite well.\n",
    "\n",
    "- Once we've created our target number of trees, we'll be left with a Random forest filled with a diverse set of decision trees that are trained on different sets of data, and also look at different subsets of features to make predictions. This amount of diversity among the trees in our forest will make for a model that is extremely resilient to noisy data, thus reducing the chance of overfitting.\n",
    "\n",
    "- In the case of a single decision tree, or even a forest where all trees focus on all the same predictors, we can expect to get the model to almost always get these false signal examples wrong. Why? Because the model will have learned to treat column 2 as a \"star player\" of sorts. When column 2 provides false signal, our model will fall for it, and get the prediction wrong.\n",
    "\n",
    "- Because this is an ensemble algorithm, the model is naturally resistant to noise and variance in the data, and generally tends to perform quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A decision tree is just a series of IF-ELSE statements (rules).\n",
    "\n",
    "- A bagged tree is different from a decision tree, in that it only receives a portion of our data, randomly selecting observations with replacement, and a random subsample of our predictor variables (columns).\n",
    "\n",
    "- A random forest is a collection of bagged trees, trained on different subsamples of our observations and predictor variables.\n",
    "\n",
    "- The benefit of a random forest is that all of the trees are trained on different sets of data, and looks at different subsets of features to make predictions. This amount of diversity among the trees in our forest will make for a model that is extremely resilient to noisy data, thus reducing the chance of overfitting. Due to the \"ensemble\" or \"combination\" of models, the overall model resists overfitting to excess noise and variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
